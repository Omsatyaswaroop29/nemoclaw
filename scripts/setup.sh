#!/bin/bash

# NemoClaw Setup Script
# Automatically detects your GPU and configures the optimal backend

set -e  # Exit on any error

# Colors for pretty output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Banner
echo -e "${CYAN}"
echo "  _   _                       ____ _                 "
echo " | \ | | ___ _ __ ___   ___  / ___| | __ ___      __ "
echo " |  \| |/ _ \ '_ \` _ \ / _ \| |   | |/ _\` \ \ /\ / / "
echo " | |\  |  __/ | | | | | (_) | |___| | (_| |\ V  V /  "
echo " |_| \_|\___|_| |_| |_|\___/ \____|_|\__,_| \_/\_/   "
echo -e "${NC}"
echo -e "${PURPLE}Run OpenClaw locally for free, forever, with complete privacy.${NC}"
echo ""

# Get script directory
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"

# Check for required tools
echo -e "${BLUE}[1/6] Checking prerequisites...${NC}"

check_command() {
    if ! command -v $1 &> /dev/null; then
        echo -e "${RED}Error: $1 is not installed.${NC}"
        echo "Please install $1 and try again."
        exit 1
    fi
    echo -e "  ${GREEN}âœ“${NC} $1 found"
}

check_command docker
check_command curl

# Check Docker is running
if ! docker info &> /dev/null; then
    echo -e "${RED}Error: Docker is not running.${NC}"
    echo "Please start Docker and try again."
    exit 1
fi
echo -e "  ${GREEN}âœ“${NC} Docker is running"

# Check for NVIDIA GPU and Container Toolkit
echo ""
echo -e "${BLUE}[2/6] Detecting GPU...${NC}"

GPU_DETECTED=false
VRAM_GB=0

if command -v nvidia-smi &> /dev/null; then
    # Get GPU info
    GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
    VRAM_MB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    VRAM_GB=$((VRAM_MB / 1024))
    
    echo -e "  ${GREEN}âœ“${NC} NVIDIA GPU detected: ${CYAN}$GPU_NAME${NC}"
    echo -e "  ${GREEN}âœ“${NC} VRAM: ${CYAN}${VRAM_GB}GB${NC}"
    GPU_DETECTED=true
    
    # Check NVIDIA Container Toolkit
    if docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi &> /dev/null; then
        echo -e "  ${GREEN}âœ“${NC} NVIDIA Container Toolkit working"
    else
        echo -e "${YELLOW}Warning: NVIDIA Container Toolkit not configured properly.${NC}"
        echo "Install it with: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"
        read -p "Continue anyway? (y/N) " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    fi
else
    echo -e "${YELLOW}Warning: No NVIDIA GPU detected or nvidia-smi not available.${NC}"
    echo "NemoClaw requires an NVIDIA GPU with at least 16GB VRAM."
    read -p "Continue anyway? (y/N) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

# Determine backend based on VRAM
echo ""
echo -e "${BLUE}[3/6] Selecting optimal configuration...${NC}"

BACKEND="llamacpp"  # Default to llama.cpp (works with less VRAM)
QUANTIZATION="Q4_K_XL"
COMPOSE_FILE="docker-compose.llamacpp.yml"

if [ "$VRAM_GB" -ge 24 ]; then
    echo -e "  ${GREEN}âœ“${NC} 24GB+ VRAM detected - using ${CYAN}vLLM with full BF16 model${NC}"
    BACKEND="vllm"
    QUANTIZATION="bf16"
    COMPOSE_FILE="docker-compose.vllm.yml"
elif [ "$VRAM_GB" -ge 16 ]; then
    echo -e "  ${GREEN}âœ“${NC} 16GB+ VRAM detected - using ${CYAN}llama.cpp with Q4_K_XL quantization${NC}"
else
    echo -e "${YELLOW}  âš  Less than 16GB VRAM - using aggressive quantization${NC}"
    QUANTIZATION="Q4_K_M"
fi

echo -e "  Backend: ${CYAN}$BACKEND${NC}"
echo -e "  Quantization: ${CYAN}$QUANTIZATION${NC}"

# Create .env file
echo ""
echo -e "${BLUE}[4/6] Creating configuration...${NC}"

cat > "$PROJECT_DIR/.env" << EOF
# NemoClaw Configuration
# Generated by setup.sh on $(date)

# Server port (OpenClaw will connect to this)
NEMOCLAW_PORT=8001

# Backend selection (auto-detected)
NEMOCLAW_BACKEND=$BACKEND

# Model quantization (auto-detected based on VRAM)
NEMOCLAW_QUANTIZATION=$QUANTIZATION

# Maximum context length (tokens)
# Nemotron supports up to 1M, but lower values use less VRAM
NEMOCLAW_MAX_CONTEXT=131072

# GPU memory utilization (0.0-1.0)
# Lower this if you get OOM errors
NEMOCLAW_GPU_MEMORY_UTILIZATION=0.9

# Hugging Face token (optional, for gated models)
# HF_TOKEN=your_token_here
EOF

echo -e "  ${GREEN}âœ“${NC} Created .env configuration file"

# Create symlink to main docker-compose file
ln -sf "docker/$COMPOSE_FILE" "$PROJECT_DIR/docker-compose.yml"
echo -e "  ${GREEN}âœ“${NC} Linked docker-compose.yml to $COMPOSE_FILE"

# Download model (for llama.cpp)
if [ "$BACKEND" = "llamacpp" ]; then
    echo ""
    echo -e "${BLUE}[5/6] Downloading model...${NC}"
    echo -e "${YELLOW}This will download ~20GB. It only happens once.${NC}"
    
    read -p "Download model now? (Y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Nn]$ ]]; then
        docker compose -f "$PROJECT_DIR/docker/$COMPOSE_FILE" --profile download up model-downloader
        echo -e "  ${GREEN}âœ“${NC} Model downloaded successfully"
    else
        echo -e "${YELLOW}  Skipping download. Run 'docker compose --profile download up model-downloader' later.${NC}"
    fi
else
    echo ""
    echo -e "${BLUE}[5/6] Model will be downloaded on first run...${NC}"
    echo -e "${YELLOW}Note: vLLM downloads the model automatically (this takes 10-20 minutes on first run).${NC}"
fi

# Final instructions
echo ""
echo -e "${BLUE}[6/6] Setup complete!${NC}"
echo ""
echo -e "${GREEN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
echo -e "${GREEN}NemoClaw is ready to launch!${NC}"
echo -e "${GREEN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
echo ""
echo -e "To start NemoClaw:"
echo -e "  ${CYAN}cd $PROJECT_DIR${NC}"
echo -e "  ${CYAN}docker compose up -d${NC}"
echo ""
echo -e "To configure OpenClaw:"
echo -e "  ${CYAN}./scripts/configure-openclaw.sh${NC}"
echo ""
echo -e "To check status:"
echo -e "  ${CYAN}curl http://localhost:8001/health${NC}"
echo ""
echo -e "To view logs:"
echo -e "  ${CYAN}docker compose logs -f${NC}"
echo ""
echo -e "${PURPLE}Happy local inferencing! ðŸ ${NC}"
